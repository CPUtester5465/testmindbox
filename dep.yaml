#по поставленному тз я выделил 2е задачи:
#1) Высокая доступность развертывания модулей на уровне узла и на уровне зоны доступности.
#2) Эффективное распределение ресурсов с защитой от перераспределения

#Первая задача уже частично решена в интернете, нам как раз нужно распределение на 3 зоны, так что возьмём шаблон из примера

apiVersion: apps/v1
kind: Deployment
metadata:
  name: demo
spec:
  selector:
    matchLabels:
      app: store
  replicas: 3
  strategy:
  type: RollingUpdate
  template:
    metadata:
      labels:
        app: store
    spec:
      affinity:
        podAntiAffinity:                                            #В "podAntiAffinity:" этом файле yaml указывается AntiAffinity узла.
          preferredDuringSchedulingIgnoredDuringExecution:          #позволяет по возможности назначать модули pod разным зонам доступности, если количество модулей pod больше, чем количество зон доступности.
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - store
            topologyKey: "failure-domain.beta.kubernetes.io/zone"
      containers:
      - name: redis-server
        image: redis:3.2-alpine
    spec:
  containers:
  - name: app
    image: images.my-company.example/app:v4
    resources:
      requests:
        memory: "128Mi"                                            #Для достижения наименьших затрат ограничемся минимально возможным реквестом
        cpu: "100m"                                                #Аналогично с CPU
      limits:
        memory: "128Mi"                                            #Согласно условию память стабильна, так что лимит будет совпадать с запросом
        cpu: "500m"                                                #Из-за скачка потребления лимит по cpu сделаем больше 

---
apiVersion: apps/v1
kind: ResoureQuotas                                                #Используем ResoureQuotas для ограничения запроса ресурсов, чтобы в случае непредвиденного всплеска подтверждения не положить ноду
metadata:                                                          #от него можно отказаться, если использовать НЕ БОЛЬШЕ ОДНОГО ПОДА НА ОДНУ НОДУ
    name: demo
spec:
    hard:
        requests.cpu: 400m
        requests.memory: 512mib
        limits.cpu: 2000m
        limits.memory: 512mib
---

#Для грамотрной реализации HPA используем сервер метрик Kubernetes
#Предположим, что сам сервер c включенным horizontal-pod-autoscaler-use-rest-clients у нас уже развёрнут. 
#Этот параметр включен по умолчанию в Kubernetes 1.9. GKE 1.9 поставляется с предварительно установленным сервером метрик.

apiVersion: apps/v1
kind: HorizontalPodAutoscaler
metadata:
  name: demo
spec:
  scaleTargetRef:
    apiVersion: extensions/v1beta1
    kind: Deployment
    name: demo
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      targetAverageUtilization: 80
  - type: Resource
    resource:
      name: memory
      targetAverageValue: 128Mi      #для экономии пространства(и как задано по условию) по памяти таргет поставим на 128

---
#Для ещё большего увеличения надёжности добавим CPA автомасштабирование на основе специальных показателей(по запросам)
#Для реализации данной функции нам необходим некоторый сервис (например Prometheus) - который будет собирать показатели приложений и сохранять их
#И  k8s-prometheus-adapter, который дополняет Custom Metrics API Kubernetes показателями, предоставленными сборщиком.
#После настройки сервера Custom metrics мы должны получить общее количество запросов в секунду из Custom Metrics API
apiVersion: apps/v1
kind: CustomPodAutoscaler
metadata:
  name: demo
spec:
  scaleTargetRef:
    apiVersion: extensions/v1beta1
    kind: Deployment
    name: demo
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Pods
    pods:
      metricName: http_requests
      targetAverageValue: 100          #придумаем данное значение, т.к. не имеем точной информации
---

